import {
  pipeline,
  type AutomaticSpeechRecognitionPipeline,
  env,
} from "@xenova/transformers";
import { distance as levenshteinDistance } from "fastest-levenshtein";

// ğŸ”§ å¼ºåˆ¶é…ç½®ï¼šç¦ç”¨æœ¬åœ°æ–‡ä»¶ç³»ç»ŸæŸ¥æ‰¾ï¼Œå¼ºåˆ¶ä½¿ç”¨æµè§ˆå™¨ç¼“å­˜
// è¿™å¯¹ Electron æ¸²æŸ“è¿›ç¨‹è‡³å…³é‡è¦ï¼Œé˜²æ­¢å®ƒå°è¯•è°ƒç”¨ fs.readFile
env.allowLocalModels = false;
env.useBrowserCache = true;

// 1. å®šä¹‰å¤šç»´åº¦è¯„åˆ†ç»“æœç±»å‹
export type ScoreResult = {
  score: number;          // ç»¼åˆæ€»åˆ†
  accuracy: number;       // å‡†ç¡®åº¦ (æ‹¼å†™/éŸ³ç´ ç›¸ä¼¼åº¦)
  completeness: number;   // å®Œæ•´åº¦ (é•¿åº¦æ¯”ä¾‹)
  fluency: number;        // æµåˆ©åº¦ (åŸºäºå†—ä½™åº¦æƒ©ç½š)
  recognizedText: string; // è¯†åˆ«å‡ºçš„æ–‡æœ¬
  referenceText: string;  // æ ‡å‡†æ–‡æœ¬
};

export type ProgressCallback = (data: {
  status: string;
  file?: string;
  progress?: number;
  loaded?: number;
  total?: number;
}) => void;

/**
 * TransformersScoringService
 * çº¯å‰ç«¯è¯­éŸ³è¯„åˆ†æœåŠ¡ï¼Œè¿è¡Œåœ¨ Electron æ¸²æŸ“è¿›ç¨‹ä¸­ã€‚
 * ä½¿ç”¨ Wav2Vec2 æ¨¡å‹è¿›è¡Œ ASRï¼Œå¹¶åŸºäºç¼–è¾‘è·ç¦»ç®—æ³•è®¡ç®—å¤šç»´åº¦è¯„åˆ†ã€‚
 */
export class TransformersScoringService {
  private static instance: TransformersScoringService;
  private asrPipeline: AutomaticSpeechRecognitionPipeline | null = null;
  private loadingPromise: Promise<void> | null = null;

  // ğŸ’¡ æ¨¡å‹é€‰æ‹©å»ºè®®ï¼š
  // "base": ä¸‹è½½å¿« (çº¦ 200MB)ï¼Œé€‚åˆå¼€å‘è°ƒè¯•
  // "large": ç²¾åº¦é«˜ (çº¦ 1.2GB)ï¼Œé€‚åˆç”Ÿäº§ç¯å¢ƒ
  private readonly modelName = "Xenova/wav2vec2-base-960h";
  // private readonly modelName = "Xenova/wav2vec2-large-960h-lv60-self";

  private constructor() { }

  static getInstance() {
    if (!TransformersScoringService.instance) {
      TransformersScoringService.instance = new TransformersScoringService();
    }
    return TransformersScoringService.instance;
  }

  /**
   * åˆå§‹åŒ– Pipeline (æ‡’åŠ è½½)
   */
  async init(onProgress?: ProgressCallback) {
    if (this.asrPipeline) return;
    if (this.loadingPromise) {
      await this.loadingPromise;
      return;
    }

    // ä¼˜å…ˆä½¿ç”¨ WebGPU åŠ é€Ÿ (M1/M2 èŠ¯ç‰‡æ”¯æŒæä½³)
    const device =
      typeof navigator !== "undefined" && (navigator as any).gpu
        ? "webgpu"
        : "wasm";

    console.log(`[Transformers] Loading model ${this.modelName} using ${device}...`);

    this.loadingPromise = pipeline("automatic-speech-recognition", this.modelName, {
      device,
      progress_callback: (data: any) => {
        onProgress?.(data);
      },
    })
      .then((asr) => {
        this.asrPipeline = asr as AutomaticSpeechRecognitionPipeline;
        console.log("[Transformers] Model loaded successfully.");
      })
      .catch((err) => {
        console.error("[Transformers] Failed to load model:", err);
        this.loadingPromise = null; // å…è®¸é‡è¯•
        throw err;
      });

    await this.loadingPromise;
  }

  /**
   * æ ¸å¿ƒè¯„åˆ†æ–¹æ³•
   */
  async score(audioBlob: Blob, referenceText: string): Promise<ScoreResult> {
    await this.init();

    if (!this.asrPipeline) {
      throw new Error("ASR pipeline not ready");
    }

    // å°† Blob è½¬ä¸º URL ä¾› pipeline ä½¿ç”¨
    const audioUrl = URL.createObjectURL(audioBlob);

    try {
      // 1. æ‰§è¡Œ ASR è¯†åˆ«
      const output = await this.asrPipeline(audioUrl, {
        return_timestamps: "word", // è™½ç„¶è¿™é‡Œæš‚æœªç”¨åˆ°æ—¶é—´æˆ³ï¼Œä½†ä¿ç•™é…ç½®ä»¥ä¾¿æœªæ¥æ‰©å±•
        chunk_length_s: 30,        // å¤„ç†é•¿éŸ³é¢‘
      });

      const recognizedText = (output?.text ?? "").trim();

      // 2. æ–‡æœ¬å½’ä¸€åŒ– (è½¬å°å†™ï¼Œå»æ ‡ç‚¹)
      const normRef = normalize(referenceText);
      const normRec = normalize(recognizedText);

      // 3. è®¡ç®—å¤šç»´åº¦åˆ†æ•°
      let accuracy = 0;
      let completeness = 0;
      let fluency = 0;
      let overallScore = 0;

      // å¦‚æœæ ‡å‡†æ–‡æœ¬ä¸ºç©ºï¼Œæ— æ³•è¯„åˆ†
      if (normRef.length === 0) {
        if (normRec.length === 0) overallScore = 100; // éƒ½ç©ºåˆ™æ»¡åˆ†
      } else {
        // --- ç®—æ³•é€»è¾‘ ---

        // A. å‡†ç¡®åº¦ (Accuracy): åŸºäº Levenshtein ç¼–è¾‘è·ç¦»
        const dist = levenshteinDistance(normRef, normRec);
        const maxLen = Math.max(normRef.length, normRec.length, 1);
        accuracy = Math.max(0, Math.round((1 - dist / maxLen) * 100));

        // B. å®Œæ•´åº¦ (Completeness): åŸºäºé•¿åº¦æ¯”ä¾‹
        // è¯»å¾—è¶Šå®Œæ•´ï¼Œé•¿åº¦è¶Šæ¥è¿‘ã€‚å¦‚æœè¯»å°‘äº†æ‰£åˆ†ï¼Œè¯»å¤šäº†ä¸æ‰£åˆ†ã€‚
        const lengthRatio = Math.min(1, normRec.length / (normRef.length || 1));
        completeness = Math.round(lengthRatio * 100);

        // C. æµåˆ©åº¦ (Fluency): æ¨¡æ‹Ÿç®—æ³•
        // å¦‚æœè¯†åˆ«å‡ºçš„æ–‡æœ¬æ¯”åŸæ–‡é•¿å¾ˆå¤šï¼Œè¯´æ˜æœ‰é‡å¤ã€åœé¡¿è¯æˆ–å™ªéŸ³
        fluency = accuracy;
        if (normRec.length > normRef.length * 1.3) {
          // æƒ©ç½šè¿‡åº¦å•°å—¦
          fluency = Math.max(0, fluency - 15);
        } else {
          // ç»™äºˆä¸€ç‚¹å¥–åŠ±åˆ†ï¼Œé¼“åŠ±è‡ªä¿¡
          fluency = Math.min(100, fluency + 5);
        }

        // D. æ€»åˆ† (Weighted Average)
        // æƒé‡ï¼šå‡†ç¡®åº¦ 60%, å®Œæ•´åº¦ 20%, æµåˆ©åº¦ 20%
        overallScore = Math.round(
          accuracy * 0.6 + completeness * 0.2 + fluency * 0.2
        );
      }

      return {
        score: overallScore,
        accuracy,
        completeness,
        fluency,
        recognizedText,
        referenceText,
      };

    } finally {
      // 4. æ¸…ç†å†…å­˜
      URL.revokeObjectURL(audioUrl);
    }
  }
}

// --- è¾…åŠ©å‡½æ•° ---

function normalize(text: string): string {
  return text
    .toLowerCase()
    // ç§»é™¤é™¤å­—æ¯ã€æ•°å­—ã€ç©ºæ ¼ä»¥å¤–çš„æ‰€æœ‰å­—ç¬¦ (å»æ ‡ç‚¹)
    .replace(/[^a-z0-9\s]/g, "")
    // å°†å¤šä¸ªè¿ç»­ç©ºæ ¼åˆå¹¶ä¸ºä¸€ä¸ª (æ³¨æ„è¿™é‡Œæ˜¯ \s ä¸æ˜¯ \\s)
    .replace(/\s+/g, " ")
    .trim();
}
